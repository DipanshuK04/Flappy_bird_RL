# -*- coding: utf-8 -*-
"""FinalSARSA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LT1tSokHiUb9Y368BDP46jyl5HmZjn7S
"""

#final Sarsa
import gym
import numpy as np
!pip install gym-minigrid
import random
from matplotlib import pyplot as plt

num_episodes=200
k_steps=200
epsilon=1
gamma=0.99
alpha=0.5
env = gym.make('MiniGrid-Empty-6x6-v0')
env.reset()
Q={}
def epsilon_greedy(epsilon,state_space):
  if np.random.rand() < epsilon:
    action = np.random.randint(0,3)
  else:
    action = np.argmax(Q[state_space])
  return action
# print('epoch starts')
for i in range(num_episodes):
    #print(i)
    env.reset()
    # c_state=env.agent_pos
    # direction = env.agent_dir
    # state_space=(c_state[0],c_state[1],direction)
    # if state_space not in Q:    #even if we can write inital state space 1,1,0
    #   Q[state_space]=[0,0,0]
    # if np.random.rand() < epsilon:
    #     action = np.random.randint(0,3)
    #   else:
    #     action = np.argmax(Q[state])
    action_in=epsilon_greedy(epsilon,(1,1,0))
    done=False
    steps=1
    # while steps < k_steps:
    # while not done or steps > k_steps:
    policy = []

    while not done:
        steps+=1
        c_state=env.agent_pos
        # print(c_state)
        direction = env.agent_dir
        # print(direction)
        state_space=(c_state[0],c_state[1],direction)
        if state_space not in Q:    #even if we can write inital state space 1,1,0
            Q[state_space]=[0,0,0]


        n_obs,reward,done,truncation,info = env.step(action_in)

        next_state = env.agent_pos
        # print(done)
        direction2 = env.agent_dir
        state_space_2 = (next_state[0],next_state[1],direction2)

        if state_space_2 not in Q:
            Q[state_space_2]=[0,0,0]

        action_fi=epsilon_greedy(epsilon,state_space_2)
        policy.append(action_fi)

        Q[state_space][action_in] = Q[state_space][action_in] + alpha*(reward + gamma*Q[state_space_2][action_fi] - Q[state_space][action_in] )
        action_in = action_fi
        epsilon=(-i/100) + 1

print(policy)
env = gym.make('MiniGrid-Empty-6x6-v0')
env.reset()
episode_reward = 0
epsilon=0
for i in range(10):
    env.reset()
    # env.render()
    done=False
    action_in=epsilon_greedy(epsilon,(1,1,0))
# for i in range(len(policy)):
    while not done:
        c_state=env.agent_pos
        # print(c_state)
        direction = env.agent_dir
        # print(direction)
        state_space=(c_state[0],c_state[1],direction)
        # if state_space not in Q:    #even if we can write inital state space 1,1,0
        #     Q[state_space]=[0,0,0]


        n_obs,reward,done,truncation,info = env.step(action_in)

        next_state = env.agent_pos
        # print(done)
        direction2 = env.agent_dir
        state_space_2 = (next_state[0],next_state[1],direction2)

        # if state_space_2 not in Q:
        #     Q[state_space_2]=[0,0,0]

        action_fi=epsilon_greedy(epsilon,state_space_2)
        action_in = action_fi
        # a = policy[i]
        # n_obs,rew,done,trunc,info = env.step(a)
        # episode_reward += rew
# print(episode_reward)
# plt.title("MiniGrid-Empty-8x8-v0 using SARSA Algorithm")
# plt.plot(epds,returns)
# plt.xlabel("Number of episodes")
# plt.ylabel("Reward at each episode")